{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework: Cluster validation indices\n",
    "\n",
    "| Student Name         | Student-ID |\n",
    "|----------------------|------------|\n",
    "| Marco Di Francesco   | 100632815  |\n",
    "| Loreto García Tejada | 100643862  |\n",
    "| György Bence Józsa   | 100633270  |\n",
    "| József-Hunor Jánosi  | 100516724  |\n",
    "| Sara-Jane Bittner    | 100498554  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "_Learning goal: To study different cluster validation indices on different datasets and different\n",
    "clusterings._\n",
    "\n",
    "In this task, you should study two internal clustering validation indices, **Silhouette index\n",
    "(SI)** and **Davies-Bouldin index (DB)**, and one external index, **Normalized Mutual\n",
    "Information (NMI)**, the version by Strehl and Ghosh, 2003 (see the slides of lecture 5).\n",
    "\n",
    "Load two data sets, “balls.txt” and “spirals.txt”. Both are two-dimensional data, where\n",
    "the third feature component (“class”) contains the ground-truth labels. Remember to discard\n",
    "the label while running the clustering algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a) Cluster “balls.txt” with i) $K$-means and ii) hierarchical single linkage clustering, both\n",
    "using the Euclidean distance measure.\n",
    "\n",
    "Use values $K = 2, \\dots , 5$ in $K$-means and similarly cut the dendrogram in $2, \\dots , 5$\n",
    "clusters. Plot the data points with different colors to visualize all your clustering results.\n",
    "\n",
    "Determine the optimal number of clusters for both methods using all three indices SI,\n",
    "DB and NMI. Report the results as a table.\n",
    "\n",
    "Which clustering method and $K$ value seem to be the best for the data i) based on the\n",
    "validation indices and ii) by visual observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "balls = pd.read_csv('balls.txt')\n",
    "balls.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics_gen(df, df_noClass, cluster, pred_labels, metrics_dict):\n",
    "    \n",
    "    metrics_dict['cluster_n'].append(cluster)\n",
    "    metrics_dict['SI'].append(metrics.silhouette_score(df_noClass, pred_labels, metric='euclidean'))\n",
    "    metrics_dict['DB'].append(metrics.davies_bouldin_score(df_noClass, pred_labels))\n",
    "    #metrics_dict['NMI'].append(metrics.normalized_mutual_info_score(df, kmeans.labels_, average_method='euclidean'))\n",
    "    \n",
    "    #external evaluation NMI\n",
    "    metrics_dict['NMI'].append(metrics.normalized_mutual_info_score(df['class'], pred_labels)) \n",
    "\n",
    "    return metrics_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#kmeans\n",
    "balls_k_dict = {\n",
    "        \"cluster_n\": [],\n",
    "        \"SI\": [],\n",
    "        \"DB\": [],\n",
    "        \"NMI\": []\n",
    "    }\n",
    "\n",
    "balls_noClass = balls.drop(labels=\"class\", axis=1)\n",
    "for i in range(2,6):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(balls_noClass)\n",
    "\n",
    "    balls_k_dict = compute_metrics_gen(balls,balls_noClass,i, kmeans.labels_, balls_k_dict)\n",
    "\n",
    "    plt.scatter(balls_noClass['X'], balls_noClass['Y'], c=kmeans.labels_)\n",
    "    plt.title('K-means, K value = %d' %i)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "balls_metrics = pd.DataFrame.from_dict(balls_k_dict)\n",
    "balls_metrics.set_index(\"cluster_n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Balls - hierarchical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#hieracical\n",
    "balls_h_dict = {\n",
    "        \"cluster_n\": [],\n",
    "        \"SI\": [],\n",
    "        \"DB\": [],\n",
    "        \"NMI\": []\n",
    "    }\n",
    "\n",
    "for i in range(2,6):\n",
    "    agClus = AgglomerativeClustering(n_clusters=i, linkage=\"single\",affinity='euclidean' )\n",
    "    agClus.fit(balls_noClass)   \n",
    "\n",
    "    balls_h_dict = compute_metrics_gen(balls,balls_noClass,i, agClus.labels_, balls_h_dict)\n",
    "\n",
    "    plt.scatter(balls_noClass['X'], balls_noClass['Y'], c=agClus.labels_)\n",
    "    plt.title('hierarchical single linkage clustering, n_clusters = %d' %i)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Z = linkage(balls_noClass, 'single')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#kmeans\n",
    "balls_metrics = pd.DataFrame.from_dict(balls_k_dict)\n",
    "print('kmeans:')\n",
    "print(balls_metrics.set_index(\"cluster_n\"))\n",
    "\n",
    "# Hierachical\n",
    "\n",
    "print('Single Linkage:')\n",
    "balls_h_metrics = pd.DataFrame.from_dict(balls_h_dict)\n",
    "print(balls_h_metrics.set_index(\"cluster_n\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer\n",
    "Which clustering method and $K$ value seem to be the best for the data...\n",
    "\n",
    "i)  ...based on the validation indices\n",
    "    The best value for each of the index is as follows: for **SI** the **SI** range is $[-1,1]$, the higher the value the better the clustering, on the other hand for **DB** the lower the value the better, and in **NMI** the best possible value is 1.\n",
    "\n",
    "|                               | $\\boldsymbol{K}$-means | Hierarchical clustering |\n",
    "|-------------------------------|------------------------|-------------------------|\n",
    "| Silhouette index              | 3 clusters             | 3 clusters              |\n",
    "| Davies-Bouldin index          | 3 clusters             | 3 clusters              |\n",
    "| Normalized mutual information | 3 clusters             | 3 clusters              |\n",
    "\n",
    "Overall, both clusters propose 3 clusters as the best amount of clusters coherent for all three validation indices. However, if you take a closer look the hierarchical single linkage clustering shows a way lower **DB** index for more than 3 clusters. Compared to kmeans, where the **DB** index raises drastically with more than 3 clusters. This indicates that a wrong choice of the number of clusters, the hierarchical clustering method loses less correctness over $K$-means. This is due to the fact that while $K$-means creates a new centroid, turning a large portion of points in a cluster into another cluster, hierarchical clustering assigns only outlier points to the additional claster.\n",
    "\n",
    "ii) ...by visual observation?\n",
    "    Regarding the $K$-means the best number of clusters visually is 3. Here the clusters are most compact in themselves and have the widest distance inter-cluster. With 2 clusters, the clusters show drastically higher intra-cluster. Further, for more than 3 clusters, clusters become less distinct and previous compact clusters are divided into two for example.\n",
    "    Regarding hierarchical clustering, visually 3 clusters are the best as well. Similar to the $K$-means, with 2 clusters the clusters show drastically higher intra clusters.  Further, for more than 3 clusters, small parts of existing compact clusters are split.\n",
    "    However, the split is less drastic for more clusters regarding the hierarchical single linkage clustering. Therefore, hierarchical clustering is the better forming clustering methods based on the plots.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sprirals - k-means"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b) Repeat all steps of a) for “spirals.txt”."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spirals = pd.read_csv('spirals.txt')\n",
    "spirals.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spirals_noClass = spirals.drop(labels=\"class\", axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "spiral_k_dict = {\n",
    "        \"cluster_n\": [],\n",
    "        \"SI\": [],\n",
    "        \"DB\": [],\n",
    "        \"NMI\": []\n",
    "    }\n",
    "for i in range(2,6):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(spirals_noClass)\n",
    "\n",
    "    spiral_k_dict = compute_metrics_gen(spirals,spirals_noClass,i, kmeans.labels_, spiral_k_dict)\n",
    "\n",
    "    plt.scatter(spirals_noClass['X'], spirals_noClass['Y'], c=kmeans.labels_)\n",
    "    plt.title('K-means, K value = %d' %i)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#kmeans\n",
    "spirals_k_metrics = pd.DataFrame.from_dict(spiral_k_dict)\n",
    "print('kmeans:')\n",
    "print(spirals_k_metrics.set_index(\"cluster_n\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer\n",
    "Which clustering method and $K$ value seem to be the best for the data...\n",
    "\n",
    "i)  ...based on the validation indices\n",
    "\n",
    "The $K$-means is not performing as well as the hierarchical clustering. Here it is also not that clear which amount of clusters is the best.\n",
    "\n",
    "\n",
    "|                               | $\\boldsymbol{K}$-means | Hierarchical clustering |\n",
    "|-------------------------------|------------------------|-------------------------|\n",
    "| Silhouette index              | 3 clusters             | 2 clusters              |\n",
    "| Davies-Bouldin index          | 4 clusters             | 5 clusters              |\n",
    "| Normalized mutual information | 5 clusters             | 3 clusters              |\n",
    "\n",
    "For the spirals.txt the single linkage clustering the metrics doesn’t represent correctly the classification, only **NMI** is a good indicator in this case of what is the best number of clusters.\n",
    "\n",
    "ii) ...by visual observation?\n",
    "\n",
    "Regarding k means the clustering for every amount of cluster is not viable. There is no clear distinction between spirals and clusters. This was hinted to by the fact that the different indices disagreed on the optimal number of clusters.\n",
    "\n",
    "Regarding hierarchical single linkage clustering, the clustering with 3 clusters shows the best performance. Here the three clusters form 3 distinct spirals. For less than 3 clusters, 2 spirals are forming one cluster. While for more than 3 clusters, the most far ends of a spirals clustered to one cluster are split to new ones.\n",
    "\n",
    "Overall, regarding the plots the hierarchical single linkage clustering is performing best for the spirals data sets. Here the clustering results in more distinct spirals and clustering.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sprirals - hierarchical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Z = linkage(spirals_noClass, 'single')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spiral_h_dict = {\n",
    "        \"cluster_n\": [],\n",
    "        \"SI\": [],\n",
    "        \"DB\": [],\n",
    "        \"NMI\": []\n",
    "    }\n",
    "\n",
    "for i in range(2,6):\n",
    "    agClus = AgglomerativeClustering(n_clusters=i, linkage=\"single\",affinity='euclidean' )\n",
    "    agClus.fit(spirals_noClass)   \n",
    "\n",
    "    spiral_h_dict = compute_metrics_gen(spirals,spirals_noClass,i, agClus.labels_, spiral_h_dict)\n",
    "    plt.scatter(spirals_noClass['X'], spirals_noClass['Y'], c=agClus.labels_)\n",
    "    plt.title('hierarchical single linkage clustering, n_clusters = %d' %i)\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#kmeans\n",
    "spirals_k_metrics = pd.DataFrame.from_dict(spiral_k_dict)\n",
    "print('kmeans:')\n",
    "print(spirals_k_metrics.set_index(\"cluster_n\"))\n",
    "\n",
    "# Hierachical\n",
    "\n",
    "print('Single Linkage:')\n",
    "spirals_h_metrics = pd.DataFrame.from_dict(spiral_h_dict)\n",
    "print(balls_h_metrics.set_index(\"cluster_n\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Explain and analyze your observations. Which index captured the performance of the\n",
    "clustering algorithm most accurately? Why some indices might have failed to reflect\n",
    "good performance?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer\n",
    "\n",
    "The performance of the indices depends on what we mean for a good clustering algorithm, there may be 2 cases: one in which we want to have an index the gives an high value for the best number of clusters with and a low value for the wrong number of clusters; the other we may want to have just the most number of datapoints in the same cluster, giving little importance if there are clusters that target only at an outlier.\n",
    "\n",
    "- **Targeting the biggest number of datapoints clustered together**: in this case the index that best captured the clustering algorithms was **NMI**. This is because it considers the ground truth labels (giving also importance to the number of clusters we have), while the other 2 indices consider the distances from the clusters.\n",
    "More in detail we can analyze the behavior in the 2 datasets:\n",
    "    - ball dataset: it gave 1.00 to both algorithms with 3 clusters, as there were no missed points\n",
    "    - spiral dataset: the **NMI** values of $K$-means were very low, correctly indicating that it is fundamentally incapable of clustering the dataset accurately. In hierarchical clustering values were much higher, closer to 1, which indicated that that method captured the topology of the dataset rather well. With $K$-means, the values were pretty uniform, while the hierarchical values indicated a clear best choice for parameters.\n",
    "- **Targeting best number of clusters**: the best index changed depending on the dataset. Considering in detail the two datasets:\n",
    "    - balls dataset: in the case of spirals dataset for $K$-means clustering **SI** gave was easier to reason with because the values in all the cases are bad and similar with each other, while for **DB** the values changed, hinting that 5 clusters was the best option\n",
    "    - spiral dataset: in both cases the indices show very low values hinting in both cases that both clustering algorithms are wrong, so having bad results. This is given by how the indices work, because the look at the centroid, so in this case the **NMI** metric would have given better results.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#kmeans\n",
    "balls_metrics = pd.DataFrame.from_dict(balls_k_dict)\n",
    "print('balls kmeans:')\n",
    "print(balls_metrics.set_index(\"cluster_n\"))\n",
    "\n",
    "# Hierachical\n",
    "\n",
    "print('balls Single Linkage:')\n",
    "balls_h_metrics = pd.DataFrame.from_dict(balls_h_dict)\n",
    "print(balls_h_metrics.set_index(\"cluster_n\"))\n",
    "\n",
    "#kmeans\n",
    "spirals_k_metrics = pd.DataFrame.from_dict(spiral_k_dict)\n",
    "print('Spirals kmeans:')\n",
    "print(spirals_k_metrics.set_index(\"cluster_n\"))\n",
    "\n",
    "# Hierachical\n",
    "\n",
    "print('Spirals Single Linkage:')\n",
    "spirals_h_metrics = pd.DataFrame.from_dict(spiral_h_dict)\n",
    "print(spirals_h_metrics.set_index(\"cluster_n\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e771d7cf59df27081d1b1a7d008817c75355cfee57678e9b400438e0926c14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}